# -*- coding: utf-8 -*-
"""Legion_Daily_Wallet Balance.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OiXrqESmYuv1w7-fLGgNiHWJarhS2cvy

Install environment management tools for API keys
"""
import requests
import os
import pandas as pd
from dotenv import load_dotenv

# ALCHEMY_API_KEY_TREASURE

import requests
load_dotenv('/content/gdrive/MyDrive/vars.env')

ALCHEMY_API_KEY_treasure = os.environ['ALCHEMY_API_KEY_TREASURE']


url = f"https://eth-mainnet.alchemyapi.io/v2/{ALCHEMY_API_KEY_TREASURE}"


# payload = {
#     "id": 1,
#     "jsonrpc": "2.0",
#     "method": "alchemy_getAssetTransfers",
#     "params": [
#         {
#             "fromBlock": "0x0",
#             "toBlock": "latest",
#             "category": ["erc20"],
#             "withMetadata": False,
#             "excludeZeroValue": True,
#             "maxCount": "0x3e8",
#             "fromAddress": "0xa8b0a855BE21568B93f32805b244158Cc61AD006"
#         }
#     ]
# }
# headers = {
#     "Accept": "application/json",
#     "Content-Type": "application/json"
# }

# response = requests.post(url, json=payload, headers=headers)

# print(response.text)

"""Define Functions
---
Define the functions for API calls. 

get_transfers_wo_pgkey: this is called first and does not take a 'page_key' input.

get_transfers_w_pgkey: can be called once a 'page_key' has been obtained
"""

ALCHEMY_API_KEY_TREASURE = os.getenv('ALCHEMY_API_KEY_TREASURE')
url = f"https://arb-mainnet.g.alchemy.com/v2/{ALCHEMY_API_KEY_TREASURE}"

# print(ALCHEMY_API_KEY_TREASURE)

def get_transfers_w_pgkey(_PAGE_KEY):
  PAGE_KEY = _PAGE_KEY
  payload = {
        "id": 1,
        "jsonrpc": "2.0",
        "method": "alchemy_getAssetTransfers",
        "params": [
            {
                "fromBlock": "0x0",
                "toBlock": "latest",
                "contractAddresses": ["0xfE8c1ac365bA6780AEc5a985D989b327C27670A1"],
                "category": ["erc721"],
                "withMetadata": True,
                "excludeZeroValue": False,
                "maxCount": "0x3e8",
                "pageKey": f"{PAGE_KEY}"
            }
        ]
    }
  headers = {
      "Accept": "application/json",
      "Content-Type": "application/json"
    }

  response = requests.post(url, json=payload, headers=headers)
  return response

def get_transfers_wo_pgkey():
  payload = {
      "id": 1,
      "jsonrpc": "2.0",
      "method": "alchemy_getAssetTransfers",
      "params": [
          {
              "fromBlock": "0x0",
              "toBlock": "latest",
              "contractAddresses": ["0xfE8c1ac365bA6780AEc5a985D989b327C27670A1"],
              "category": ["erc721"],
              "withMetadata": True,
              "excludeZeroValue": False,
              "maxCount": "0x3e8"
          }
      ]
  }
  headers = {
      "Accept": "application/json",
      "Content-Type": "application/json"
  }

  response = requests.post(url, json=payload, headers=headers)
  return response


# print(response.text)



"""Get All Transfer Data
---
Use a loop that checks for a page_key to capture all transfer data. If there is no additional data pg_key will be empty and will return false.
"""

response=get_transfers_wo_pgkey()
page_key = response.json()['result']['pageKey']
result = response.json()['result']['transfers']
concat_num =0
df_new = pd.DataFrame()
df_transfers = pd.json_normalize(result)
df_new = df_transfers
# while df_new.shape[0] == 1000:
while page_key:
  response=get_transfers_w_pgkey(page_key)
  if 'error' in response.json(): 
    try:
      response=get_transfers_w_pgkey(page_key)
    except:
      print(response.json()['error']['code'])
  try:
    page_key = response.json()['result']['pageKey']
    # response=get_transfers_w_pgkey(page_key)
    result = response.json()['result']['transfers']
  except:
    page_key = False
    print(response.text)
    print("page key not found")
  

  df_new = pd.json_normalize(result)

  df_transfers=pd.concat([df_transfers, df_new], axis=0)
  concat_num = concat_num + 1
  if concat_num % 25 == 0:
    print(f"Iteration Number: {concat_num}")

# df_transfers
# response = get_transfers()
# response.json()

# df = pd.json_normalize(response.json()['result']['transfers'])

df_transfers





df_transfers.shape[0]

df_transfers



# df_transfers = pd.json_normalize(result)
df_transfers['blockNum'] = df_transfers['blockNum'].apply(int, base=16)
df_transfers['erc721TokenId']=df_transfers['erc721TokenId'].apply(int, base=16)
df_transfers = df_transfers[['blockNum', 'hash', 'from', 'to', 'erc721TokenId', 'tokenId', 'asset', 'rawContract.address', 'metadata.blockTimestamp']]
df_transfers

df_transfers = df_transfers[['blockNum', 'hash', 'from', 'to', 'erc721TokenId', 'asset', 'rawContract.address', 'metadata.blockTimestamp']]

df_transfers

df_transfer_to = df_transfers[['metadata.blockTimestamp', 'blockNum', 'hash', 'to', 'erc721TokenId','asset', 'rawContract.address']]
df_transfer_to['value'] = 1
df_transfer_to=df_transfer_to.rename(columns = {'metadata.blockTimestamp':'timestamp', 'blockNum':'block_num', 'to':'address', 'rawContract.address':'contract_address'})
df_transfer_from = df_transfers[['metadata.blockTimestamp', 'blockNum', 'hash', 'from', 'erc721TokenId','asset', 'rawContract.address']]
df_transfer_from['value'] = -1
df_transfer_from=df_transfer_from.rename(columns = {'metadata.blockTimestamp':'timestamp', 'blockNum':'block_num', 'from':'address', 'rawContract.address':'contract_address'})

df_transfer_from.info()

# df_transfer_to.merge(df_transfer_from, how='left', left_on='to', right_on='from')
pd_transfer_w_value = pd.concat([df_transfer_to, df_transfer_from], axis=0)

pd_transfer_w_value.info()

addresses = pd.DataFrame(pd_transfer_w_value['address'].unique())
days = pd.to_datetime(pd_transfer_w_value['timestamp']).dt.date.unique()
days = pd.date_range(start=days.min(), end=days.max(), freq="D")
# days.max()
# days.min()

df_test = pd.DataFrame(
    {
    'day': days
    # 'addresses': addresses
    }
)
addresses['key'] = 1
df_test['key']=1
result=df_test.merge(addresses, on='key').drop(columns=['key'])

r=result.reset_index().rename(columns={'day':'date', 0:'address'})
r['date'] = pd.to_datetime(r['date']).dt.date
r

r.columns

addresses.size

# grp=pd_transfer_w_value.groupby(['timestamp', 'address']).sum()
grp=pd_transfer_w_value.reset_index()
grp['day'] = pd.to_datetime(grp['timestamp']).dt.date
grp.sort_values('timestamp', inplace=True)
grp.set_index('timestamp', inplace=True)
grp=grp.drop(columns=['index', 'block_num', 'hash', 'asset', 'erc721TokenId'])


# grp['daily_change']=grp.groupby('day').sum()
daily_change = grp.groupby(['day', 'address']).sum().reset_index()

daily_change.reset_index().head(50)

grp.info()

daily_change_all_days = r.merge(daily_change, how='left', left_on=['date','address'], right_on=['day','address']).fillna(0)
daily_change_all_days=daily_change_all_days[['date', 'address', 'value']]

daily_change_all_days[daily_change_all_days['address'] == '0x92e1543420434a47c8548dc7aae92651127c889f'].tail(30)
# daily_change_all_days

"""Daily Wallet Balances & Net Movement
---
Get the net change in tokens and wallet balance for each address
"""

daily_change_all_days['cumsum'] = daily_change_all_days.groupby(['address'])['value'].cumsum()
daily_change_all_days

daily_change_all_days[daily_change_all_days['address'] == '0x16e95f6bf27f0b16b19ad7d07635e69c49897272'].head(30)

wallet_bal_by_day = grp.drop(columns=['timestamp', 'block_num', 'hash', 'erc721TokenId', 'asset'])
wallet_bal_by_day=wallet_bal_by_day[['day', 'contract_address','address', 'value', 'cumsum']]
wallet_bal_by_day

# wallet_bal_by_day['cumsum'] = wallet_bal_by_day.groupby(['address'])['value'].cumsum()

# wallet_bal_by_day.tail(35)

# wallet_bal_by_day['cumsum']=wallet_bal_by_day.groupby(['day', 'contract_address', 'address'])['value'].cumsum()

# wallet_bal_by_day

daily_change_all_days.tail(100)

"""Unique Addresses with LGN Tokens
---
Get the number of addresses holding LGN tokens each day
"""

daily_change_all_days[(daily_change_all_days['cumsum'] > 0)].groupby('date').size()